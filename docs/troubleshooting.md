# vLLM API ì„œë²„ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

## ê°œìš”

ì´ ë¬¸ì„œëŠ” vLLM API ì„œë²„ ìš´ì˜ ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ë¬¸ì œë“¤ê³¼ í•´ê²° ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ë¬¸ì œ ìœ í˜•ë³„ë¡œ ë‹¨ê³„ì ì¸ ì§„ë‹¨ ë° í•´ê²° ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ëª©ì°¨

1. [ì„œë²„ ì‹œì‘ ë¬¸ì œ](#ì„œë²„-ì‹œì‘-ë¬¸ì œ)
2. [ëª¨ë¸ ë¡œë”© ë¬¸ì œ](#ëª¨ë¸-ë¡œë”©-ë¬¸ì œ)
3. [GPU ê´€ë ¨ ë¬¸ì œ](#gpu-ê´€ë ¨-ë¬¸ì œ)
4. [ë©”ëª¨ë¦¬ ë¬¸ì œ](#ë©”ëª¨ë¦¬-ë¬¸ì œ)
5. [ì„±ëŠ¥ ë¬¸ì œ](#ì„±ëŠ¥-ë¬¸ì œ)
6. [ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ](#ë„¤íŠ¸ì›Œí¬-ë¬¸ì œ)
7. [API ì‘ë‹µ ë¬¸ì œ](#api-ì‘ë‹µ-ë¬¸ì œ)
8. [Docker ê´€ë ¨ ë¬¸ì œ](#docker-ê´€ë ¨-ë¬¸ì œ)
9. [Kubernetes ê´€ë ¨ ë¬¸ì œ](#kubernetes-ê´€ë ¨-ë¬¸ì œ)
10. [ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§](#ë¡œê¹…-ë°-ëª¨ë‹ˆí„°ë§)

## ì„œë²„ ì‹œì‘ ë¬¸ì œ

### 1. í¬íŠ¸ ë°”ì¸ë”© ì‹¤íŒ¨

**ì¦ìƒ:**
```
OSError: [Errno 98] Address already in use
```

**ì›ì¸:** ì§€ì •ëœ í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘

**í•´ê²° ë°©ë²•:**
```bash
# 1. í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
lsof -i :8000
netstat -tlnp | grep :8000

# 2. í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
kill -9 <PID>

# 3. ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
export PORT=8001
python app/main.py

# 4. ì‹œìŠ¤í…œ ì¬ë¶€íŒ… í›„ ì¬ì‹œë„ (í•„ìš”ì‹œ)
sudo reboot
```

### 2. í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½

**ì¦ìƒ:**
```
KeyError: 'MODEL_NAME'
ValueError: Environment variable not set
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. í™˜ê²½ ë³€ìˆ˜ í™•ì¸
env | grep -i model

# 2. .env íŒŒì¼ ì¡´ì¬ í™•ì¸
ls -la .env

# 3. .env.exampleì—ì„œ ë³µì‚¬
cp .env.example .env

# 4. í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
export HOST=0.0.0.0
export PORT=8000
```

### 3. ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ëˆ„ë½

**ì¦ìƒ:**
```
ModuleNotFoundError: No module named 'vllm'
ImportError: cannot import name 'AsyncLLMEngine'
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
which python
echo $VIRTUAL_ENV

# 2. ì˜ì¡´ì„± ì¬ì„¤ì¹˜
pip install -r requirements.txt

# 3. vLLM ìˆ˜ë™ ì„¤ì¹˜
pip install vllm

# 4. CUDA ë²„ì „ í˜¸í™˜ì„± í™•ì¸
nvidia-smi
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

## ëª¨ë¸ ë¡œë”© ë¬¸ì œ

### 1. ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ

**ì¦ìƒ:**
```
FileNotFoundError: Model not found at /path/to/model
OSError: Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ëª¨ë¸ ê²½ë¡œ í™•ì¸
ls -la /app/models/
echo $MODEL_PATH

# 2. Hugging Face ìºì‹œ í™•ì¸
ls -la ~/.cache/huggingface/

# 3. ëª¨ë¸ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')
model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-chat-hf')
"

# 4. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
curl -I https://huggingface.co

# 5. í† í° ì„¤ì • (private ëª¨ë¸ì˜ ê²½ìš°)
export HUGGINGFACE_HUB_TOKEN=your_token_here
```

### 2. ëª¨ë¸ ê¶Œí•œ ë¬¸ì œ

**ì¦ìƒ:**
```
PermissionError: [Errno 13] Permission denied
OSError: You are trying to access a gated repo
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. íŒŒì¼ ê¶Œí•œ í™•ì¸
ls -la /app/models/
stat /app/models/model_name/

# 2. ê¶Œí•œ ìˆ˜ì •
sudo chown -R $(whoami):$(whoami) /app/models/
chmod -R 755 /app/models/

# 3. Hugging Face í† í° ì„¤ì •
huggingface-cli login

# 4. Docker ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ ì‹¤í–‰ì‹œ
docker run -v ~/.cache/huggingface:/root/.cache/huggingface your-image
```

### 3. ëª¨ë¸ í˜•ì‹ í˜¸í™˜ì„± ë¬¸ì œ

**ì¦ìƒ:**
```
ValueError: Unsupported model architecture
RuntimeError: Model format not supported by vLLM
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ì§€ì› ëª¨ë¸ ëª©ë¡ í™•ì¸
python -c "from vllm import ModelRegistry; print(ModelRegistry.get_supported_archs())"

# 2. ëª¨ë¸ ë³€í™˜ (í•„ìš”ì‹œ)
python convert_model.py --input /path/to/original --output /path/to/converted

# 3. í˜¸í™˜ ëª¨ë¸ë¡œ êµì²´
export MODEL_NAME=microsoft/DialoGPT-medium

# 4. vLLM ë²„ì „ ì—…ë°ì´íŠ¸
pip install --upgrade vllm
```

## GPU ê´€ë ¨ ë¬¸ì œ

### 1. GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

**ì¦ìƒ:**
```
RuntimeError: No CUDA GPUs are available
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. GPU ìƒíƒœ í™•ì¸
nvidia-smi
lspci | grep -i nvidia

# 2. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ ìƒíƒœ í™•ì¸
nvidia-driver-version
dpkg -l | grep nvidia

# 3. CUDA ì„¤ì¹˜ í™•ì¸
nvcc --version
echo $CUDA_HOME

# 4. Dockerì—ì„œ GPU ì ‘ê·¼ í™•ì¸
docker run --gpus all nvidia/cuda:11.8-base nvidia-smi

# 5. ë“œë¼ì´ë²„ ì¬ì„¤ì¹˜ (í•„ìš”ì‹œ)
sudo apt purge nvidia-* -y
sudo apt autoremove -y
sudo apt install nvidia-driver-470 -y
sudo reboot
```

### 2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ:**
```
torch.cuda.OutOfMemoryError: CUDA out of memory
RuntimeError: Unable to allocate memory on device
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# 2. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¡°ì •
export GPU_MEMORY_UTILIZATION=0.7  # ê¸°ë³¸ê°’: 0.9

# 3. ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
export MAX_MODEL_LEN=2048  # ê¸°ë³¸ê°’: 4096

# 4. ë°°ì¹˜ í¬ê¸° ì¡°ì •
export MAX_NUM_BATCHED_TOKENS=4096

# 5. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo fuser -v /dev/nvidia*
kill -9 <GPU_PID>

# 6. GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
python -c "
import torch
torch.cuda.empty_cache()
torch.cuda.ipc_collect()
"
```

### 3. ë©€í‹° GPU ì„¤ì • ë¬¸ì œ

**ì¦ìƒ:**
```
RuntimeError: Tensor parallel size exceeds number of available GPUs
AssertionError: tensor_parallel_size must be divisible by world_size
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìˆ˜ í™•ì¸
nvidia-smi --list-gpus
python -c "import torch; print(torch.cuda.device_count())"

# 2. í…ì„œ ë³‘ë ¬í™” í¬ê¸° ì¡°ì •
export TENSOR_PARALLEL_SIZE=2  # GPU ìˆ˜ì— ë§ê²Œ ì¡°ì •

# 3. Ray í´ëŸ¬ìŠ¤í„° ì„¤ì • í™•ì¸
ray status
ray list nodes

# 4. GPU ê°€ì‹œì„± ì„¤ì •
export CUDA_VISIBLE_DEVICES=0,1,2,3

# 5. ë¶„ì‚° ì„¤ì • í™•ì¸
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1  # InfiniBand ë¹„í™œì„±í™”
```

## ë©”ëª¨ë¦¬ ë¬¸ì œ

### 1. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ:**
```
MemoryError: Unable to allocate memory
OSError: [Errno 12] Cannot allocate memory
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
free -h
top -o %MEM
ps aux --sort=-%mem | head

# 2. ë©”ëª¨ë¦¬ ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
sudo systemctl stop unnecessary-service
killall chrome firefox

# 3. ìŠ¤ì™‘ ê³µê°„ í™•ì¸ ë° ì¶”ê°€
swapon --show
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 4. ëª¨ë¸ ë¡œë”© ë°©ì‹ ë³€ê²½
export LOAD_IN_8BIT=true
export LOAD_IN_4BIT=true

# 5. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
python -c "
import gc
gc.collect()
"
```

### 2. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜

**ì¦ìƒ:**
- ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
- ì„œë²„ ì‘ë‹µ ì†ë„ ì ì§„ì  ì €í•˜

**í•´ê²° ë°©ë²•:**
```bash
# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
python -c "
import psutil
import time
while True:
    mem = psutil.virtual_memory()
    print(f'Memory: {mem.percent}% used')
    time.sleep(60)
"

# 2. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
pip install memory-profiler
python -m memory_profiler app/main.py

# 3. ì£¼ê¸°ì  ì¬ì‹œì‘ ì„¤ì •
# crontab -e
# 0 2 * * * /usr/bin/systemctl restart vllm-api

# 4. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ íŠœë‹
export PYTHONHASHSEED=0
export MALLOC_MMAP_THRESHOLD_=131072
```

## ì„±ëŠ¥ ë¬¸ì œ

### 1. ì‘ë‹µ ì†ë„ ì €í•˜

**ì¦ìƒ:**
- API ì‘ë‹µ ì‹œê°„ ì¦ê°€
- ë†’ì€ ëŒ€ê¸° ì‹œê°„

**ì§„ë‹¨:**
```bash
# 1. ì‘ë‹µ ì‹œê°„ ì¸¡ì •
curl -w "@curl-format.txt" -o /dev/null -s "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"model":"meta-llama/Llama-2-7b-chat-hf","messages":[{"role":"user","content":"Hello"}]}'

# curl-format.txt ë‚´ìš©:
# time_namelookup:    %{time_namelookup}\n
# time_connect:       %{time_connect}\n
# time_appconnect:    %{time_appconnect}\n
# time_pretransfer:   %{time_pretransfer}\n
# time_redirect:      %{time_redirect}\n
# time_starttransfer: %{time_starttransfer}\n
# time_total:         %{time_total}\n

# 2. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
htop
iotop
nvidia-smi -l 1
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ì›Œì»¤ ìˆ˜ ì¡°ì •
export WORKERS=4  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ

# 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
export MAX_NUM_BATCHED_TOKENS=8192
export MAX_NUM_SEQS=256

# 3. KV ìºì‹œ ìµœì í™”
export ENABLE_PREFIX_CACHING=true
export KV_CACHE_DTYPE="fp8"

# 4. ì¶”ë¡  ì—”ì§„ ì„¤ì • íŠœë‹
export USE_V2_BLOCK_MANAGER=true
export PREEMPTION_MODE="recompute"
```

### 2. ë†’ì€ CPU ì‚¬ìš©ë¥ 

**ì¦ìƒ:**
```
CPU usage consistently above 90%
High system load average
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. CPU ì‚¬ìš©ëŸ‰ ë¶„ì„
top -H -p $(pgrep -f vllm)
perf top -p $(pgrep -f vllm)

# 2. ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8

# 3. CPU ì¹œí™”ì„± ì„¤ì •
taskset -c 0-7 python app/main.py

# 4. ìš°ì„ ìˆœìœ„ ì¡°ì •
nice -n -10 python app/main.py
```

### 3. ë””ìŠ¤í¬ I/O ë³‘ëª©

**ì¦ìƒ:**
```
High disk wait time (iowait)
Slow model loading
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
df -h
iotop -o

# 2. ëª¨ë¸ì„ SSDë¡œ ì´ë™
sudo mount /dev/nvme0n1 /app/models
export MODEL_PATH=/app/models

# 3. ë””ìŠ¤í¬ ìºì‹œ ì„¤ì •
echo 3 > /proc/sys/vm/drop_caches
echo 'vm.swappiness=10' >> /etc/sysctl.conf

# 4. tmpfs ì‚¬ìš© (ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œ ê²½ìš°)
sudo mount -t tmpfs -o size=32G tmpfs /tmp/models
```

## ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ

### 1. ì—°ê²° ì‹œê°„ ì´ˆê³¼

**ì¦ìƒ:**
```
requests.exceptions.Timeout: HTTPSConnectionPool
curl: (28) Operation timed out
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸
ping -c 4 localhost
telnet localhost 8000

# 2. ë°©í™”ë²½ í™•ì¸
sudo ufw status
sudo iptables -L

# 3. íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¡°ì •
export REQUEST_TIMEOUT=300
export KEEPALIVE_TIMEOUT=60

# 4. í”„ë¡ì‹œ ì„¤ì • í™•ì¸
echo $HTTP_PROXY
echo $HTTPS_PROXY
unset HTTP_PROXY HTTPS_PROXY  # í•„ìš”ì‹œ
```

### 2. ëŒ€ì—­í­ ë¬¸ì œ

**ì¦ìƒ:**
- ëŒ€ìš©ëŸ‰ ì‘ë‹µ ì „ì†¡ ì§€ì—°
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ëŠê¹€

**í•´ê²° ë°©ë²•:**
```bash
# 1. ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ í™•ì¸
iperf3 -s  # ì„œë²„
iperf3 -c server_ip  # í´ë¼ì´ì–¸íŠ¸

# 2. TCP íŠœë‹
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
sudo sysctl -p

# 3. ì••ì¶• í™œì„±í™”
export ENABLE_GZIP=true

# 4. ì²­í¬ í¬ê¸° ì¡°ì •
export CHUNK_SIZE=8192
```

## API ì‘ë‹µ ë¬¸ì œ

### 1. HTTP 500 ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜

**ì¦ìƒ:**
```json
{
  "error": {
    "message": "Internal server error",
    "type": "internal_error"
  }
}
```

**ì§„ë‹¨ ë° í•´ê²°:**
```bash
# 1. ë¡œê·¸ í™•ì¸
tail -f logs/app.log
grep -i error logs/app.log

# 2. ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í™œì„±í™”
export LOG_LEVEL=DEBUG
export TRACEBACK_ENABLED=true

# 3. í—¬ìŠ¤ì²´í¬ í™•ì¸
curl http://localhost:8000/health

# 4. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
sudo systemctl restart vllm-api
# ë˜ëŠ”
docker restart vllm-api-container
```

### 2. ì˜ëª»ëœ JSON ì‘ë‹µ

**ì¦ìƒ:**
```
json.decoder.JSONDecodeError: Expecting value
Invalid JSON format in response
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ì‘ë‹µ í˜•ì‹ ê²€ì¦
curl -v http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"meta-llama/Llama-2-7b-chat-hf","messages":[{"role":"user","content":"test"}]}'

# 2. JSON ìœ íš¨ì„± ê²€ì‚¬
echo '$response' | jq .

# 3. ì¸ì½”ë”© í™•ì¸
export PYTHONIOENCODING=utf-8
locale

# 4. ì‘ë‹µ í¬ê¸° ì œí•œ í™•ì¸
export MAX_RESPONSE_SIZE=32768
```

### 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë¬¸ì œ

**ì¦ìƒ:**
- ìŠ¤íŠ¸ë¦¬ë°ì´ ì¤‘ê°„ì— ëŠê¹€
- ë¶ˆì™„ì „í•œ SSE ì´ë²¤íŠ¸

**í•´ê²° ë°©ë²•:**
```bash
# 1. ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
curl -N http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"meta-llama/Llama-2-7b-chat-hf","messages":[{"role":"user","content":"Count to 10"}],"stream":true}'

# 2. ë²„í¼ë§ ë¹„í™œì„±í™”
export PYTHONUNBUFFERED=1
export STREAMING_TIMEOUT=60

# 3. í”„ë¡ì‹œ ì„¤ì • í™•ì¸ (nginx)
# nginx.confì—ì„œ:
# proxy_buffering off;
# proxy_cache off;
# proxy_read_timeout 300s;
```

## Docker ê´€ë ¨ ë¬¸ì œ

### 1. ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹¤íŒ¨

**ì¦ìƒ:**
```
docker: Error response from daemon: could not select device driver
Container exited with code 125
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. Docker ë¡œê·¸ í™•ì¸
docker logs container_name
docker events

# 2. GPU ëŸ°íƒ€ì„ í™•ì¸
docker run --rm --gpus all nvidia/cuda:11.8-base nvidia-smi

# 3. nvidia-container-runtime ì„¤ì¹˜
sudo apt-get install nvidia-container-runtime
sudo systemctl restart docker

# 4. Docker daemon ì„¤ì • í™•ì¸
cat /etc/docker/daemon.json
sudo systemctl status docker

# 5. ê¶Œí•œ ë¬¸ì œ í•´ê²°
sudo usermod -aG docker $USER
newgrp docker
```

### 2. ë³¼ë¥¨ ë§ˆìš´íŠ¸ ë¬¸ì œ

**ì¦ìƒ:**
```
docker: Error response from daemon: invalid mount config
Permission denied when accessing mounted files
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ë³¼ë¥¨ ê¶Œí•œ í™•ì¸
ls -la /host/path
docker exec container_name ls -la /container/path

# 2. SELinux ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì • (CentOS/RHEL)
sudo chcon -Rt svirt_sandbox_file_t /host/path

# 3. ì˜¬ë°”ë¥¸ ë§ˆìš´íŠ¸ ë¬¸ë²• ì‚¬ìš©
docker run -v /absolute/host/path:/container/path:rw image_name

# 4. ì‚¬ìš©ì ID ë§¤í•‘
docker run --user $(id -u):$(id -g) image_name
```

### 3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ

**ì¦ìƒ:**
```
docker: Error response from daemon: network not found
Container cannot reach external services
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ë„¤íŠ¸ì›Œí¬ ëª©ë¡ í™•ì¸
docker network ls

# 2. ì»¨í…Œì´ë„ˆ ë„¤íŠ¸ì›Œí¬ ì •ë³´
docker inspect container_name | grep -A 20 NetworkSettings

# 3. ì‚¬ìš©ì ì •ì˜ ë„¤íŠ¸ì›Œí¬ ìƒì„±
docker network create vllm-network
docker run --network vllm-network image_name

# 4. DNS ì„¤ì • í™•ì¸
docker run --dns 8.8.8.8 image_name

# 5. í¬íŠ¸ ë°”ì¸ë”© í™•ì¸
docker run -p 8000:8000 image_name
netstat -tlnp | grep 8000
```

## Kubernetes ê´€ë ¨ ë¬¸ì œ

### 1. Pod ì‹œì‘ ì‹¤íŒ¨

**ì¦ìƒ:**
```
Pod stuck in Pending state
CrashLoopBackOff status
ImagePullBackOff error
```

**ì§„ë‹¨:**
```bash
# 1. Pod ìƒíƒœ í™•ì¸
kubectl get pods -n vllm-api
kubectl describe pod pod-name -n vllm-api

# 2. ì´ë²¤íŠ¸ ë¡œê·¸ í™•ì¸
kubectl get events -n vllm-api --sort-by='.lastTimestamp'

# 3. Pod ë¡œê·¸ í™•ì¸
kubectl logs pod-name -n vllm-api
kubectl logs pod-name -n vllm-api --previous
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ë¦¬ì†ŒìŠ¤ ìš”ì²­ëŸ‰ ì¡°ì •
kubectl edit deployment vllm-api-deployment -n vllm-api
# resources.requests ê°’ ê°ì†Œ

# 2. ë…¸ë“œ ìƒíƒœ í™•ì¸
kubectl get nodes
kubectl describe node node-name

# 3. ì´ë¯¸ì§€ í’€ ì •ì±… ìˆ˜ì •
kubectl patch deployment vllm-api-deployment -n vllm-api -p '{"spec":{"template":{"spec":{"containers":[{"name":"vllm-api","imagePullPolicy":"Always"}]}}}}'

# 4. Secret í™•ì¸ (private ì´ë¯¸ì§€)
kubectl get secrets -n vllm-api
kubectl create secret docker-registry regcred \
  --docker-server=your-registry.com \
  --docker-username=your-username \
  --docker-password=your-password
```

### 2. ì„œë¹„ìŠ¤ ì—°ê²° ë¬¸ì œ

**ì¦ìƒ:**
```
Service endpoints not available
Cannot connect to service from other pods
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
kubectl get svc -n vllm-api
kubectl describe svc vllm-api-service -n vllm-api

# 2. ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
kubectl get endpoints -n vllm-api
kubectl describe endpoints vllm-api-service -n vllm-api

# 3. ë ˆì´ë¸” ì…€ë ‰í„° í™•ì¸
kubectl get pods -n vllm-api --show-labels
kubectl get svc vllm-api-service -n vllm-api -o yaml | grep selector

# 4. ë„¤íŠ¸ì›Œí¬ ì •ì±… í™•ì¸
kubectl get networkpolicies -n vllm-api

# 5. DNS í…ŒìŠ¤íŠ¸
kubectl run test-pod --image=busybox --rm -it --restart=Never -- nslookup vllm-api-service.vllm-api.svc.cluster.local
```

### 3. í¼ì‹œìŠ¤í„´íŠ¸ ë³¼ë¥¨ ë¬¸ì œ

**ì¦ìƒ:**
```
PersistentVolumeClaim stuck in Pending
Volume mount failed
```

**í•´ê²° ë°©ë²•:**
```bash
# 1. PVC ìƒíƒœ í™•ì¸
kubectl get pvc -n vllm-api
kubectl describe pvc model-pvc -n vllm-api

# 2. ìŠ¤í† ë¦¬ì§€ í´ë˜ìŠ¤ í™•ì¸
kubectl get storageclass
kubectl describe storageclass storage-class-name

# 3. PV ìƒíƒœ í™•ì¸
kubectl get pv
kubectl describe pv pv-name

# 4. ë…¸ë“œ ìŠ¤í† ë¦¬ì§€ í™•ì¸
kubectl get nodes -o wide
ssh node-name "df -h"

# 5. ë™ì  í”„ë¡œë¹„ì €ë‹ ì„¤ì • í™•ì¸
kubectl get pods -n kube-system | grep provisioner
```

## ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

### 1. ë¡œê·¸ ìˆ˜ì§‘ ì„¤ì •

**ë¡œê·¸ ë ˆë²¨ ì¡°ì •:**
```bash
# í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
export LOG_LEVEL=DEBUG  # DEBUG, INFO, WARNING, ERROR

# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f logs/app.log

# ë¡œê·¸ í•„í„°ë§
grep -i "error\|exception\|failure" logs/app.log
grep -E "^$(date +%Y-%m-%d)" logs/app.log  # ì˜¤ëŠ˜ ë¡œê·¸ë§Œ
```

**êµ¬ì¡°í™”ëœ ë¡œê¹…:**
```python
# app/utils/logger.py í™•ì¸
import logging
import json

def setup_structured_logging():
    formatter = logging.Formatter(
        '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "message": "%(message)s", "module": "%(name)s"}'
    )
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)
    logging.getLogger().addHandler(handler)
```

### 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§

**ê¸°ë³¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘:**
```bash
# 1. CPU ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
ps -p $(pgrep -f vllm) -o pid,ppid,cmd,%mem,%cpu

# 2. GPU ë©”íŠ¸ë¦­
nvidia-smi --query-gpu=timestamp,name,temperature.gpu,utilization.gpu,memory.used,memory.total --format=csv -l 5

# 3. ë„¤íŠ¸ì›Œí¬ I/O
ss -tuln | grep :8000
netstat -i

# 4. ë””ìŠ¤í¬ I/O
iostat -x 1 5
```

**Prometheus ë©”íŠ¸ë¦­ ì„¤ì •:**
```python
# app/middleware/metrics.py
from prometheus_client import Counter, Histogram, Gauge

REQUEST_COUNT = Counter('requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')
GPU_MEMORY_USAGE = Gauge('gpu_memory_usage_bytes', 'GPU memory usage')
```

### 3. ì•Œë¦¼ ì„¤ì •

**ê¸°ë³¸ ì•Œë¦¼ ê·œì¹™:**
```yaml
# prometheus/alerts.yml
groups:
- name: vllm-api-alerts
  rules:
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
    for: 5m
    annotations:
      summary: High error rate detected
      
  - alert: HighGPUMemoryUsage
    expr: gpu_memory_usage_bytes / gpu_memory_total_bytes > 0.95
    for: 2m
    annotations:
      summary: GPU memory usage is too high
      
  - alert: ServiceDown
    expr: up{job="vllm-api"} == 0
    for: 1m
    annotations:
      summary: vLLM API service is down
```

**Slack ì•Œë¦¼ ì„¤ì •:**
```bash
# alertmanager.yml
route:
  group_by: ['alertname']
  receiver: 'slack-notifications'

receivers:
- name: 'slack-notifications'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts'
    title: 'vLLM API Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
```

## ë””ë²„ê¹… ë„êµ¬ ë° ê¸°ë²•

### 1. í”„ë¡œíŒŒì¼ë§

**ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§:**
```bash
# memory_profiler ì‚¬ìš©
pip install memory-profiler
python -m memory_profiler app/main.py

# line_profiler ì‚¬ìš©
pip install line_profiler
kernprof -l -v app/main.py
```

**ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§:**
```bash
# cProfile ì‚¬ìš©
python -m cProfile -o profile.stats app/main.py
python -c "import pstats; pstats.Stats('profile.stats').sort_stats('cumulative').print_stats(20)"

# py-spy ì‚¬ìš©
pip install py-spy
py-spy top --pid $(pgrep -f vllm)
py-spy record -o profile.svg --pid $(pgrep -f vllm)
```

### 2. ë„¤íŠ¸ì›Œí¬ ë””ë²„ê¹…

**íŒ¨í‚· ìº¡ì²˜:**
```bash
# tcpdump ì‚¬ìš©
sudo tcpdump -i any -w capture.pcap port 8000

# Wiresharkë¡œ ë¶„ì„
wireshark capture.pcap
```

**HTTP ìš”ì²­ ë¶„ì„:**
```bash
# mitmproxy ì‚¬ìš©
pip install mitmproxy
mitmproxy -p 8080

# í´ë¼ì´ì–¸íŠ¸ì—ì„œ í”„ë¡ì‹œ ì„¤ì •
curl --proxy localhost:8080 http://localhost:8000/health
```

### 3. ë¶„ì‚° ì¶”ì 

**OpenTelemetry ì„¤ì •:**
```python
# app/tracing.py
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)
```

## ì¼ë°˜ì ì¸ ì—ëŸ¬ ì½”ë“œ ë° í•´ê²°ì±…

### HTTP ìƒíƒœ ì½”ë“œë³„ ëŒ€ì‘

| ìƒíƒœ ì½”ë“œ | ì›ì¸ | í•´ê²° ë°©ë²• |
|----------|------|-----------|
| 400 | ì˜ëª»ëœ ìš”ì²­ í˜•ì‹ | JSON ìŠ¤í‚¤ë§ˆ í™•ì¸, í•„ìˆ˜ í•„ë“œ ê²€ì¦ |
| 401 | ì¸ì¦ ì‹¤íŒ¨ | API í‚¤ í™•ì¸, í† í° ìœ íš¨ì„± ê²€ì¦ |
| 403 | ê¶Œí•œ ì—†ìŒ | ì‚¬ìš©ì ê¶Œí•œ í™•ì¸, RBAC ì„¤ì • ê²€í†  |
| 404 | ë¦¬ì†ŒìŠ¤ ì—†ìŒ | ì—”ë“œí¬ì¸íŠ¸ URL í™•ì¸, ë¼ìš°íŒ… ì„¤ì • ê²€í†  |
| 413 | ìš”ì²­ í¬ê¸° ì´ˆê³¼ | MAX_REQUEST_SIZE ì„¤ì • ì¡°ì • |
| 429 | ìš”ì²­ í•œë„ ì´ˆê³¼ | ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ì„¤ì • í™•ì¸, í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ê°„ê²© ì¡°ì • |
| 500 | ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ | ë¡œê·¸ í™•ì¸, ì˜ˆì™¸ ì²˜ë¦¬ ê°œì„  |
| 502 | ê²Œì´íŠ¸ì›¨ì´ ì˜¤ë¥˜ | ì—…ìŠ¤íŠ¸ë¦¼ ì„œë²„ ìƒíƒœ í™•ì¸, ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì • |
| 503 | ì„œë¹„ìŠ¤ ë¶ˆê°€ | ì„œë²„ ê³¼ë¶€í•˜ í™•ì¸, ë¦¬ì†ŒìŠ¤ ì¦ì„¤ |
| 504 | ê²Œì´íŠ¸ì›¨ì´ íƒ€ì„ì•„ì›ƒ | íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¦ê°€, ì„±ëŠ¥ ìµœì í™” |

### vLLM íŠ¹í™” ì˜¤ë¥˜

**í…ì„œ ê´€ë ¨ ì˜¤ë¥˜:**
```python
# RuntimeError: Expected all tensors to be on the same device
# í•´ê²°: GPU ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± í™•ì¸
export CUDA_VISIBLE_DEVICES=0
```

**í† í¬ë‚˜ì´ì € ì˜¤ë¥˜:**
```python
# ValueError: Tokenizer not found for model
# í•´ê²°: í† í¬ë‚˜ì´ì € ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
```

**quantization ì˜¤ë¥˜:**
```python
# RuntimeError: quantization not supported
# í•´ê²°: ì§€ì›ë˜ëŠ” quantization ë°©ë²• ì‚¬ìš©
export QUANTIZATION="bitsandbytes"  # ë˜ëŠ” "gptq", "awq"
```

## ìë™í™”ëœ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸

### ì„œë¹„ìŠ¤ ìë™ ì¬ì‹œì‘
```bash
#!/bin/bash
# auto_restart.sh

SERVICE_NAME="vllm-api"
MAX_RETRIES=3
RETRY_COUNT=0

while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
    if ! curl -f http://localhost:8000/health > /dev/null 2>&1; then
        echo "ì„œë¹„ìŠ¤ ì‘ë‹µ ì—†ìŒ. ì¬ì‹œì‘ ì‹œë„ $((RETRY_COUNT + 1))/$MAX_RETRIES"
        
        # Docker í™˜ê²½
        if command -v docker &> /dev/null; then
            docker restart $SERVICE_NAME
        # Systemd í™˜ê²½
        elif command -v systemctl &> /dev/null; then
            systemctl restart $SERVICE_NAME
        # Kubernetes í™˜ê²½
        elif command -v kubectl &> /dev/null; then
            kubectl rollout restart deployment/vllm-api-deployment -n vllm-api
        fi
        
        sleep 30
        RETRY_COUNT=$((RETRY_COUNT + 1))
    else
        echo "ì„œë¹„ìŠ¤ ì •ìƒ ë™ì‘ í™•ì¸"
        exit 0
    fi
done

echo "ì„œë¹„ìŠ¤ ë³µêµ¬ ì‹¤íŒ¨. ìˆ˜ë™ ê°œì… í•„ìš”"
exit 1
```

### ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# cleanup_resources.sh

echo "ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘..."

# 1. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
echo "GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘..."
python -c "
import torch
torch.cuda.empty_cache()
print('GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ')
"

# 2. ì‹œìŠ¤í…œ ìºì‹œ ì •ë¦¬
echo "ì‹œìŠ¤í…œ ìºì‹œ ì •ë¦¬ ì¤‘..."
sync
echo 3 > /proc/sys/vm/drop_caches

# 3. ì„ì‹œ íŒŒì¼ ì •ë¦¬
echo "ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘..."
find /tmp -name "*.tmp" -mtime +1 -delete
find /var/log -name "*.log" -size +100M -exec truncate -s 50M {} \;

# 4. Docker ì •ë¦¬ (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
if command -v docker &> /dev/null; then
    echo "Docker ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘..."
    docker system prune -f
    docker volume prune -f
fi

echo "âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ"
```

## ê¸´ê¸‰ ëŒ€ì‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì‹œ ëŒ€ì‘ ìˆœì„œ

1. **ì¦‰ì‹œ í™•ì¸ ì‚¬í•­**
   - [ ] ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ (`curl http://localhost:8000/health`)
   - [ ] í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ìƒíƒœ (`ps aux | grep vllm`)
   - [ ] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ (`top`, `nvidia-smi`)
   - [ ] ë¡œê·¸ ì—ëŸ¬ í™•ì¸ (`tail -100 logs/app.log`)

2. **1ì°¨ ë³µêµ¬ ì‹œë„ (5ë¶„ ì´ë‚´)**
   - [ ] ì„œë¹„ìŠ¤ ì¬ì‹œì‘
   - [ ] GPU ë©”ëª¨ë¦¬ ì •ë¦¬
   - [ ] ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸

3. **2ì°¨ ë³µêµ¬ ì‹œë„ (15ë¶„ ì´ë‚´)**
   - [ ] ì»¨í…Œì´ë„ˆ/Pod ì¬ì‹œì‘
   - [ ] ì„¤ì • íŒŒì¼ ê²€ì¦
   - [ ] ì˜ì¡´ì„± ì„œë¹„ìŠ¤ í™•ì¸

4. **3ì°¨ ë³µêµ¬ ì‹œë„ (30ë¶„ ì´ë‚´)**
   - [ ] ë°±ì—…ì—ì„œ ë³µì›
   - [ ] ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ íŠ¸ë˜í”½ ë¼ìš°íŒ…
   - [ ] ìš´ì˜íŒ€ ì—ìŠ¤ì»¬ë ˆì´ì…˜

### ì—°ë½ì²˜ ì •ë³´

- **L1 ì§€ì›**: support-l1@company.com
- **L2 ì§€ì›**: support-l2@company.com  
- **ì˜¨ì½œ ì—”ì§€ë‹ˆì–´**: +82-10-1234-5678
- **ìŠ¬ë™ ì±„ë„**: #vllm-api-support

## ì°¸ê³  ìë£Œ

- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [CUDA ë¬¸ì œ í•´ê²° ê°€ì´ë“œ](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/)
- [Docker GPU ë¬¸ì œ í•´ê²°](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/troubleshooting.html)
- [Kubernetes ë¬¸ì œ í•´ê²°](https://kubernetes.io/docs/tasks/debug-application-cluster/)

ì´ ë¬¸ì„œëŠ” ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ë©°, ìƒˆë¡œìš´ ë¬¸ì œì™€ í•´ê²°ì±…ì´ ë°œê²¬ë˜ë©´ ì¶”ê°€ë©ë‹ˆë‹¤. ë¬¸ì œ í•´ê²°ì— ë„ì›€ì´ í•„ìš”í•˜ë©´ ìœ„ì˜ ì—°ë½ì²˜ë¡œ ë¬¸ì˜í•˜ì„¸ìš”.