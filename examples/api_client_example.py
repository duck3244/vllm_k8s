#!/usr/bin/env python3
"""
examples/api_client_example.py
vLLM API í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì˜ˆì œ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” vLLM API ì„œë²„ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
ë™ê¸°/ë¹„ë™ê¸° í˜¸ì¶œ, ìŠ¤íŠ¸ë¦¬ë°, ë°°ì¹˜ ì²˜ë¦¬ ë“±ì˜ ì˜ˆì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    python examples/api_client_example.py
    python examples/api_client_example.py --api-base http://localhost:8000
    python examples/api_client_example.py --demo streaming
    python examples/api_client_example.py --demo batch --input-file prompts.txt
"""

import argparse
import asyncio
import json
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, AsyncGenerator, Generator
import aiohttp
import requests
from dataclasses import dataclass
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class CompletionRequest:
    """ì™„ì„± ìš”ì²­ ë°ì´í„° í´ë˜ìŠ¤"""
    prompt: str
    model: str = "default"
    max_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.9
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    stop: Optional[List[str]] = None
    stream: bool = False

@dataclass
class ChatMessage:
    """ì±„íŒ… ë©”ì‹œì§€ ë°ì´í„° í´ë˜ìŠ¤"""
    role: str  # "system", "user", "assistant"
    content: str

@dataclass
class ChatRequest:
    """ì±„íŒ… ìš”ì²­ ë°ì´í„° í´ë˜ìŠ¤"""
    messages: List[ChatMessage]
    model: str = "default"
    max_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.9
    stream: bool = False

class VLLMAPIClient:
    """vLLM API í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, api_base: str = "http://localhost:8000", api_key: Optional[str] = None):
        self.api_base = api_base.rstrip('/')
        self.api_key = api_key
        self.session = requests.Session()
        self.session.timeout = 60
        
        # í—¤ë” ì„¤ì •
        self.headers = {
            "Content-Type": "application/json",
            "User-Agent": "vLLM-API-Client/1.0"
        }
        
        if api_key:
            self.headers["Authorization"] = f"Bearer {api_key}"
        
        self.session.headers.update(self.headers)
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.session.close()
    
    # ë™ê¸° ë©”ì„œë“œë“¤
    def get_models(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            response = self.session.get(f"{self.api_base}/v1/models")
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
    
    def complete(self, request: CompletionRequest) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì™„ì„± (ë™ê¸°)"""
        payload = {
            "model": request.model,
            "prompt": request.prompt,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "frequency_penalty": request.frequency_penalty,
            "presence_penalty": request.presence_penalty,
            "stream": False
        }
        
        if request.stop:
            payload["stop"] = request.stop
        
        try:
            response = self.session.post(f"{self.api_base}/v1/completions", json=payload)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"ì™„ì„± ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise
    
    def complete_stream(self, request: CompletionRequest) -> Generator[Dict[str, Any], None, None]:
        """í…ìŠ¤íŠ¸ ì™„ì„± ìŠ¤íŠ¸ë¦¬ë° (ë™ê¸°)"""
        payload = {
            "model": request.model,
            "prompt": request.prompt,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "frequency_penalty": request.frequency_penalty,
            "presence_penalty": request.presence_penalty,
            "stream": True
        }
        
        if request.stop:
            payload["stop"] = request.stop
        
        try:
            response = self.session.post(
                f"{self.api_base}/v1/completions",
                json=payload,
                stream=True
            )
            response.raise_for_status()
            
            for line in response.iter_lines(decode_unicode=True):
                if line.startswith("data: "):
                    data_str = line[6:]  # "data: " ì œê±°
                    if data_str.strip() == "[DONE]":
                        break
                    try:
                        chunk = json.loads(data_str)
                        yield chunk
                    except json.JSONDecodeError:
                        continue
                        
        except requests.exceptions.RequestException as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì™„ì„± ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise
    
    def chat(self, request: ChatRequest) -> Dict[str, Any]:
        """ì±„íŒ… ì™„ì„± (ë™ê¸°)"""
        payload = {
            "model": request.model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in request.messages],
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "stream": False
        }
        
        try:
            response = self.session.post(f"{self.api_base}/v1/chat/completions", json=payload)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"ì±„íŒ… ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise
    
    # ë¹„ë™ê¸° ë©”ì„œë“œë“¤
    async def async_get_models(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ (ë¹„ë™ê¸°)"""
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.get(f"{self.api_base}/v1/models") as response:
                    response.raise_for_status()
                    return await response.json()
            except aiohttp.ClientError as e:
                logger.error(f"ë¹„ë™ê¸° ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                raise
    
    async def async_complete(self, request: CompletionRequest) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì™„ì„± (ë¹„ë™ê¸°)"""
        payload = {
            "model": request.model,
            "prompt": request.prompt,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "frequency_penalty": request.frequency_penalty,
            "presence_penalty": request.presence_penalty,
            "stream": False
        }
        
        if request.stop:
            payload["stop"] = request.stop
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.post(f"{self.api_base}/v1/completions", json=payload) as response:
                    response.raise_for_status()
                    return await response.json()
            except aiohttp.ClientError as e:
                logger.error(f"ë¹„ë™ê¸° ì™„ì„± ìš”ì²­ ì‹¤íŒ¨: {e}")
                raise
    
    async def async_complete_stream(self, request: CompletionRequest) -> AsyncGenerator[Dict[str, Any], None]:
        """í…ìŠ¤íŠ¸ ì™„ì„± ìŠ¤íŠ¸ë¦¬ë° (ë¹„ë™ê¸°)"""
        payload = {
            "model": request.model,
            "prompt": request.prompt,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "frequency_penalty": request.frequency_penalty,
            "presence_penalty": request.presence_penalty,
            "stream": True
        }
        
        if request.stop:
            payload["stop"] = request.stop
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.post(f"{self.api_base}/v1/completions", json=payload) as response:
                    response.raise_for_status()
                    
                    async for line in response.content:
                        line_str = line.decode('utf-8').strip()
                        if line_str.startswith("data: "):
                            data_str = line_str[6:]
                            if data_str == "[DONE]":
                                break
                            try:
                                chunk = json.loads(data_str)
                                yield chunk
                            except json.JSONDecodeError:
                                continue
                                
            except aiohttp.ClientError as e:
                logger.error(f"ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì™„ì„± ìš”ì²­ ì‹¤íŒ¨: {e}")
                raise
    
    async def async_chat(self, request: ChatRequest) -> Dict[str, Any]:
        """ì±„íŒ… ì™„ì„± (ë¹„ë™ê¸°)"""
        payload = {
            "model": request.model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in request.messages],
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "stream": False
        }
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            try:
                async with session.post(f"{self.api_base}/v1/chat/completions", json=payload) as response:
                    response.raise_for_status()
                    return await response.json()
            except aiohttp.ClientError as e:
                logger.error(f"ë¹„ë™ê¸° ì±„íŒ… ìš”ì²­ ì‹¤íŒ¨: {e}")
                raise

class DemoRunner:
    """ë°ëª¨ ì‹¤í–‰ í´ë˜ìŠ¤"""
    
    def __init__(self, api_base: str, api_key: Optional[str] = None):
        self.client = VLLMAPIClient(api_base, api_key)
    
    def run_basic_demo(self):
        """ê¸°ë³¸ ì‚¬ìš©ë²• ë°ëª¨"""
        print("ğŸš€ ê¸°ë³¸ ì‚¬ìš©ë²• ë°ëª¨")
        print("=" * 50)
        
        try:
            # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
            print("1ï¸âƒ£ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘...")
            models = self.client.get_models()
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models.get('data', []))}ê°œ")
            
            for model in models.get('data', [])[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                print(f"   - {model.get('id', 'Unknown')}")
            
            # ê¸°ë³¸ ì™„ì„± ìš”ì²­
            print("\n2ï¸âƒ£ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì™„ì„±...")
            request = CompletionRequest(
                prompt="ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€",
                max_tokens=50,
                temperature=0.7
            )
            
            start_time = time.time()
            result = self.client.complete(request)
            end_time = time.time()
            
            if result.get('choices'):
                generated_text = result['choices'][0]['text']
                print(f"   í”„ë¡¬í”„íŠ¸: {request.prompt}")
                print(f"   ìƒì„± ê²°ê³¼: {generated_text.strip()}")
                print(f"   ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            
            # ì±„íŒ… ì™„ì„± ìš”ì²­
            print("\n3ï¸âƒ£ ì±„íŒ… ì™„ì„±...")
            chat_request = ChatRequest(
                messages=[
                    ChatMessage(role="system", content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
                    ChatMessage(role="user", content="íŒŒì´ì¬ìœ¼ë¡œ ê°„ë‹¨í•œ ê³„ì‚°ê¸°ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.")
                ],
                max_tokens=100
            )
            
            start_time = time.time()
            chat_result = self.client.chat(chat_request)
            end_time = time.time()
            
            if chat_result.get('choices'):
                response = chat_result['choices'][0]['message']['content']
                print(f"   ì§ˆë¬¸: íŒŒì´ì¬ìœ¼ë¡œ ê°„ë‹¨í•œ ê³„ì‚°ê¸°ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.")
                print(f"   ë‹µë³€: {response.strip()[:200]}...")
                print(f"   ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            
            print("\nâœ… ê¸°ë³¸ ë°ëª¨ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def run_streaming_demo(self):
        """ìŠ¤íŠ¸ë¦¬ë° ë°ëª¨"""
        print("ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ë°ëª¨")
        print("=" * 50)
        
        try:
            request = CompletionRequest(
                prompt="ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                max_tokens=200,
                temperature=0.8,
                stream=True
            )
            
            print(f"í”„ë¡¬í”„íŠ¸: {request.prompt}")
            print("ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ:")
            print("-" * 30)
            
            generated_text = ""
            chunk_count = 0
            start_time = time.time()
            
            for chunk in self.client.complete_stream(request):
                if chunk.get('choices'):
                    chunk_text = chunk['choices'][0].get('text', '')
                    generated_text += chunk_text
                    print(chunk_text, end='', flush=True)
                    chunk_count += 1
            
            end_time = time.time()
            
            print(f"\n{'-' * 30}")
            print(f"ì´ ì²­í¬ ìˆ˜: {chunk_count}")
            print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(generated_text)}ì")
            print(f"ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            print("\nâœ… ìŠ¤íŠ¸ë¦¬ë° ë°ëª¨ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    async def run_async_demo(self):
        """ë¹„ë™ê¸° ë°ëª¨"""
        print("âš¡ ë¹„ë™ê¸° ì²˜ë¦¬ ë°ëª¨")
        print("=" * 50)
        
        try:
            # ì—¬ëŸ¬ ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬
            prompts = [
                "íŒŒì´ì¬ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ê¸°ê³„í•™ìŠµì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì›¹ ê°œë°œì˜ ê¸°ì´ˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "ë°ì´í„°ë² ì´ìŠ¤ì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ]
            
            print(f"ë™ì‹œì— {len(prompts)}ê°œ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
            
            # ë¹„ë™ê¸° ì™„ì„± ìš”ì²­ë“¤ ìƒì„±
            tasks = []
            for prompt in prompts:
                request = CompletionRequest(
                    prompt=prompt,
                    max_tokens=80,
                    temperature=0.7
                )
                task = self.client.async_complete(request)
                tasks.append((prompt, task))
            
            start_time = time.time()
            
            # ëª¨ë“  ìš”ì²­ì„ ë™ì‹œì— ì‹¤í–‰
            results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
            
            end_time = time.time()
            
            print(f"\nğŸ“Š ê²°ê³¼ (ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ):")
            print("-" * 60)
            
            for i, ((prompt, _), result) in enumerate(zip(tasks, results)):
                print(f"\n{i+1}. ì§ˆë¬¸: {prompt}")
                
                if isinstance(result, Exception):
                    print(f"   âŒ ì˜¤ë¥˜: {result}")
                elif result.get('choices'):
                    response = result['choices'][0]['text'].strip()
                    print(f"   âœ… ë‹µë³€: {response[:100]}...")
                else:
                    print(f"   âš ï¸ ì‘ë‹µ ì—†ìŒ")
            
            print("\nâœ… ë¹„ë™ê¸° ë°ëª¨ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë¹„ë™ê¸° ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def run_batch_demo(self, input_file: Optional[str] = None):
        """ë°°ì¹˜ ì²˜ë¦¬ ë°ëª¨"""
        print("ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ë°ëª¨")
        print("=" * 50)
        
        try:
            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            if input_file and Path(input_file).exists():
                print(f"íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ: {input_file}")
                with open(input_file, 'r', encoding='utf-8') as f:
                    prompts = [line.strip() for line in f if line.strip()]
            else:
                print("ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                prompts = [
                    "ì¢‹ì€ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²•ì€?",
                    "íš¨ìœ¨ì ì¸ í•™ìŠµ ë°©ë²•ì€?",
                    "ê±´ê°•í•œ ìƒí™œìŠµê´€ì€?",
                    "ì‹œê°„ ê´€ë¦¬ì˜ í•µì‹¬ì€?",
                    "ì°½ì˜ì„±ì„ ê¸°ë¥´ëŠ” ë°©ë²•ì€?"
                ]
            
            print(f"ì²˜ë¦¬í•  í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}")
            
            results = []
            total_tokens = 0
            total_time = 0
            
            for i, prompt in enumerate(prompts, 1):
                print(f"\n[{i}/{len(prompts)}] ì²˜ë¦¬ ì¤‘: {prompt[:30]}...")
                
                request = CompletionRequest(
                    prompt=prompt,
                    max_tokens=100,
                    temperature=0.7
                )
                
                start_time = time.time()
                
                try:
                    result = self.client.complete(request)
                    end_time = time.time()
                    
                    if result.get('choices'):
                        generated_text = result['choices'][0]['text']
                        tokens_used = result.get('usage', {}).get('total_tokens', 0)
                        
                        results.append({
                            'prompt': prompt,
                            'response': generated_text.strip(),
                            'tokens': tokens_used,
                            'time': end_time - start_time
                        })
                        
                        total_tokens += tokens_used
                        total_time += end_time - start_time
                        
                        print(f"   âœ… ì™„ë£Œ ({end_time - start_time:.2f}ì´ˆ, {tokens_used}í† í°)")
                    else:
                        print(f"   âš ï¸ ì‘ë‹µ ì—†ìŒ")
                        
                except Exception as e:
                    print(f"   âŒ ì˜¤ë¥˜: {e}")
                
                # ìš”ì²­ ê°„ ê°„ê²© (API ì œí•œ ê³ ë ¤)
                time.sleep(0.1)
            
            # ê²°ê³¼ ìš”ì•½
            print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼:")
            print(f"   â€¢ ì„±ê³µí•œ ìš”ì²­: {len(results)}/{len(prompts)}")
            print(f"   â€¢ ì´ í† í° ì‚¬ìš©ëŸ‰: {total_tokens}")
            print(f"   â€¢ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"   â€¢ í‰ê·  ì‘ë‹µ ì‹œê°„: {total_time/len(results):.2f}ì´ˆ" if results else "   â€¢ í‰ê·  ì‘ë‹µ ì‹œê°„: N/A")
            
            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            output_file = f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"   â€¢ ê²°ê³¼ ì €ì¥: {output_file}")
            print("\nâœ… ë°°ì¹˜ ì²˜ë¦¬ ë°ëª¨ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def run_performance_test(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë°ëª¨"""
        print("ğŸƒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë°ëª¨")
        print("=" * 50)
        
        try:
            test_configs = [
                {"max_tokens": 50, "temperature": 0.1, "name": "ì§§ì€ ì‘ë‹µ, ë‚®ì€ ì˜¨ë„"},
                {"max_tokens": 100, "temperature": 0.7, "name": "ì¤‘ê°„ ì‘ë‹µ, ë³´í†µ ì˜¨ë„"},
                {"max_tokens": 200, "temperature": 0.9, "name": "ê¸´ ì‘ë‹µ, ë†’ì€ ì˜¨ë„"},
            ]
            
            test_prompt = "ì¸ê³µì§€ëŠ¥ê³¼ ê¸°ê³„í•™ìŠµì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            
            for config in test_configs:
                print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸: {config['name']}")
                print(f"   ì„¤ì •: max_tokens={config['max_tokens']}, temperature={config['temperature']}")
                
                # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ í‰ê·  ì„±ëŠ¥ ì¸¡ì •
                times = []
                token_counts = []
                
                for i in range(3):
                    request = CompletionRequest(
                        prompt=test_prompt,
                        max_tokens=config['max_tokens'],
                        temperature=config['temperature']
                    )
                    
                    start_time = time.time()
                    result = self.client.complete(request)
                    end_time = time.time()
                    
                    response_time = end_time - start_time
                    times.append(response_time)
                    
                    if result.get('usage'):
                        token_counts.append(result['usage'].get('total_tokens', 0))
                    
                    print(f"   ì‹¤í–‰ {i+1}: {response_time:.2f}ì´ˆ")
                
                avg_time = sum(times) / len(times)
                avg_tokens = sum(token_counts) / len(token_counts) if token_counts else 0
                tokens_per_second = avg_tokens / avg_time if avg_time > 0 else 0
                
                print(f"   ğŸ“Š í‰ê·  ì„±ëŠ¥:")
                print(f"      â€¢ ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
                print(f"      â€¢ í† í° ìˆ˜: {avg_tokens:.1f}ê°œ")
                print(f"      â€¢ í† í°/ì´ˆ: {tokens_per_second:.1f}")
            
            print("\nâœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def run_error_handling_demo(self):
        """ì˜¤ë¥˜ ì²˜ë¦¬ ë°ëª¨"""
        print("ğŸ› ï¸ ì˜¤ë¥˜ ì²˜ë¦¬ ë°ëª¨")
        print("=" * 50)
        
        test_cases = [
            {
                "name": "ë§¤ìš° ê¸´ í”„ë¡¬í”„íŠ¸",
                "request": CompletionRequest(
                    prompt="A" * 10000,  # ë§¤ìš° ê¸´ í”„ë¡¬í”„íŠ¸
                    max_tokens=10
                )
            },
            {
                "name": "ì˜ëª»ëœ ëª¨ë¸ëª…",
                "request": CompletionRequest(
                    prompt="Hello",
                    model="non-existent-model",
                    max_tokens=10
                )
            },
            {
                "name": "0ê°œ í† í° ìš”ì²­",
                "request": CompletionRequest(
                    prompt="Hello",
                    max_tokens=0
                )
            },
            {
                "name": "ì˜ëª»ëœ ì˜¨ë„ ê°’",
                "request": CompletionRequest(
                    prompt="Hello",
                    temperature=5.0,  # ë³´í†µ 0-2 ë²”ìœ„
                    max_tokens=10
                )
            }
        ]
        
        for test_case in test_cases:
            print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸: {test_case['name']}")
            
            try:
                result = self.client.complete(test_case['request'])
                print(f"   âœ… ì„±ê³µ (ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼)")
                
            except requests.exceptions.HTTPError as e:
                print(f"   âŒ HTTP ì˜¤ë¥˜: {e.response.status_code} - {e.response.reason}")
                try:
                    error_detail = e.response.json()
                    print(f"      ì„¸ë¶€ì‚¬í•­: {error_detail}")
                except:
                    pass
                    
            except requests.exceptions.ConnectionError:
                print(f"   âŒ ì—°ê²° ì˜¤ë¥˜: ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
            except requests.exceptions.Timeout:
                print(f"   âŒ ì‹œê°„ ì´ˆê³¼: ìš”ì²­ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")
                
            except Exception as e:
                print(f"   âŒ ê¸°íƒ€ ì˜¤ë¥˜: {type(e).__name__}: {e}")
        
        print("\nâœ… ì˜¤ë¥˜ ì²˜ë¦¬ ë°ëª¨ ì™„ë£Œ!")

def create_sample_prompts_file():
    """ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ìƒì„±"""
    sample_prompts = [
        "íš¨ê³¼ì ì¸ í”„ë ˆì  í…Œì´ì…˜ì„ ë§Œë“œëŠ” ë°©ë²•ì€?",
        "íŒ€ì›Œí¬ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì€?",
        "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì€?",
        "ìƒˆë¡œìš´ ê¸°ìˆ ì„ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€?",
        "ì˜ì‚¬ê²°ì •ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì€?",
        "ì°½ì˜ì  ì‚¬ê³ ë¥¼ ê¸°ë¥´ëŠ” ë°©ë²•ì€?",
        "ëª©í‘œ ì„¤ì •ì˜ ì¤‘ìš”ì„±ì€?",
        "íš¨ìœ¨ì ì¸ ë…ì„œ ë°©ë²•ì€?",
        "ê±´ê°•í•œ ìˆ˜ë©´ ìŠµê´€ì€?",
        "ì„±ê³µì ì¸ ë„¤íŠ¸ì›Œí‚¹ ë°©ë²•ì€?"
    ]
    
    filename = "sample_prompts.txt"
    with open(filename, 'w', encoding='utf-8') as f:
        for prompt in sample_prompts:
            f.write(f"{prompt}\n")
    
    print(f"âœ… ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ìƒì„±: {filename}")
    return filename

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="vLLM API í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì˜ˆì œ")
    
    parser.add_argument(
        "--api-base",
        default="http://localhost:8000",
        help="vLLM API ì„œë²„ ì£¼ì†Œ (ê¸°ë³¸ê°’: http://localhost:8000)"
    )
    
    parser.add_argument(
        "--api-key",
        help="API í‚¤ (í•„ìš”í•œ ê²½ìš°)"
    )
    
    parser.add_argument(
        "--demo",
        choices=["basic", "streaming", "async", "batch", "performance", "error", "all"],
        default="basic",
        help="ì‹¤í–‰í•  ë°ëª¨ íƒ€ì… (ê¸°ë³¸ê°’: basic)"
    )
    
    parser.add_argument(
        "--input-file",
        help="ë°°ì¹˜ ì²˜ë¦¬ìš© ì…ë ¥ íŒŒì¼ (í•œ ì¤„ì— í•˜ë‚˜ì”© í”„ë¡¬í”„íŠ¸)"
    )
    
    parser.add_argument(
        "--create-sample",
        action="store_true",
        help="ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ìƒì„±"
    )
    
    args = parser.parse_args()
    
    # ìƒ˜í”Œ íŒŒì¼ ìƒì„±
    if args.create_sample:
        create_sample_prompts_file()
        return
    
    print("ğŸš€ vLLM API í´ë¼ì´ì–¸íŠ¸ ì˜ˆì œ")
    print(f"ğŸ“¡ API ì„œë²„: {args.api_base}")
    print(f"ğŸ­ ë°ëª¨ íƒ€ì…: {args.demo}")
    print("=" * 60)
    
    # ë°ëª¨ ì‹¤í–‰ê¸° ìƒì„±
    runner = DemoRunner(args.api_base, args.api_key)
    
    try:
        if args.demo == "basic" or args.demo == "all":
            runner.run_basic_demo()
            
        if args.demo == "streaming" or args.demo == "all":
            if args.demo == "all":
                print("\n" + "=" * 60)
            runner.run_streaming_demo()
            
        if args.demo == "async" or args.demo == "all":
            if args.demo == "all":
                print("\n" + "=" * 60)
            await runner.run_async_demo()
            
        if args.demo == "batch" or args.demo == "all":
            if args.demo == "all":
                print("\n" + "=" * 60)
            runner.run_batch_demo(args.input_file)
            
        if args.demo == "performance" or args.demo == "all":
            if args.demo == "all":
                print("\n" + "=" * 60)
            runner.run_performance_test()
            
        if args.demo == "error" or args.demo == "all":
            if args.demo == "all":
                print("\n" + "=" * 60)
            runner.run_error_handling_demo()
        
        print(f"\nğŸ‰ ëª¨ë“  ë°ëª¨ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(130)
        
    except Exception as e:
        print(f"\nâŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        sys.exit(1)